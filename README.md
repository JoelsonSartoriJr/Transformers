# Transformers from Scratch

This repository implements Transformers from scratch based on the article "Attention Is All You Need" using PyTorch. The goal of this project is to understand and implement the core components of the Transformer model, which has revolutionized various natural language processing tasks.

## Article Reference
The implementation in this repository is based on the seminal paper:

- **Title**: Attention Is All You Need
- **Authors**: Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia
- **Year**: 2017
- **Link**: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

We highly recommend reading the original article to gain a deeper understanding of the Transformer model and its inner workings.

## Implementation Details
The implementation in this repository focuses on building the core components of the Transformer model, including the self-attention mechanism and the feed-forward neural network. The code is written in Python using the PyTorch library.

The main files in this repository are:

- `transformer`: This file contains the implementation of the Transformer model. It includes classes for the encoder and decoder, as well as the self-attention and feed-forward layers.


## Acknowledgments
We would like to acknowledge the authors of the "Attention Is All You Need" paper for their groundbreaking work in introducing the Transformer model. Without their contributions, this implementation would not have been possible.

## License
This repository is licensed under the [GNU](LICENSE). Feel free to use and modify the code as per your requirements.
